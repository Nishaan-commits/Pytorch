{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3070fea2",
   "metadata": {},
   "source": [
    "#### Transfer Learning for Image Classfication in PyTorch\n",
    "\n",
    "- How CNN Learns\n",
    "    - \n",
    "- Layer Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e843213",
   "metadata": {},
   "source": [
    "#### Downloading the Dataset\n",
    "\n",
    "We'll use the Oxford-IIIT Pets dataset from https://course.fast.ai/datasets. It is 37 category (breeds) pet dataset with roughly 200 images for each class. The images have a large variation in scale, pose and lighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271b5f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 812M/812M [08:21<00:00, 1.62MB/s]  \n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "download_url('https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a173872",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEOFError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtarfile\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tarfile.open(\u001b[33m'\u001b[39m\u001b[33m./oxford-iiit-pet.tgz\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr:gz\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/tarfile.py:2323\u001b[39m, in \u001b[36mTarFile.extractall\u001b[39m\u001b[34m(self, path, members, numeric_owner, filter)\u001b[39m\n\u001b[32m   2318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isdir():\n\u001b[32m   2319\u001b[39m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[32m   2320\u001b[39m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[32m   2321\u001b[39m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[32m   2322\u001b[39m         directories.append(unfiltered)\n\u001b[32m-> \u001b[39m\u001b[32m2323\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2324\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2325\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[32m   2328\u001b[39m directories.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m a: a.name, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/tarfile.py:2426\u001b[39m, in \u001b[36mTarFile._extract_one\u001b[39m\u001b[34m(self, tarinfo, path, set_attrs, numeric_owner, filter_function)\u001b[39m\n\u001b[32m   2423\u001b[39m \u001b[38;5;28mself\u001b[39m._check(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2425\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2426\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2427\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2428\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2429\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2430\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mextraction_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2432\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_fatal_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/tarfile.py:2515\u001b[39m, in \u001b[36mTarFile._extract_member\u001b[39m\u001b[34m(self, tarinfo, targetpath, set_attrs, numeric_owner, filter_function, extraction_root)\u001b[39m\n\u001b[32m   2512\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbg(\u001b[32m1\u001b[39m, tarinfo.name)\n\u001b[32m   2514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isreg():\n\u001b[32m-> \u001b[39m\u001b[32m2515\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tarinfo.isdir():\n\u001b[32m   2517\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedir(tarinfo, targetpath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/tarfile.py:2572\u001b[39m, in \u001b[36mTarFile.makefile\u001b[39m\u001b[34m(self, tarinfo, targetpath)\u001b[39m\n\u001b[32m   2570\u001b[39m     target.truncate()\n\u001b[32m   2571\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mReadError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/tarfile.py:251\u001b[39m, in \u001b[36mcopyfileobj\u001b[39m\u001b[34m(src, dst, length, exception, bufsize)\u001b[39m\n\u001b[32m    249\u001b[39m blocks, remainder = \u001b[38;5;28mdivmod\u001b[39m(length, bufsize)\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(blocks):\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     buf = \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) < bufsize:\n\u001b[32m    253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[33m\"\u001b[39m\u001b[33munexpected end of data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/gzip.py:324\u001b[39m, in \u001b[36mGzipFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merrno\u001b[39;00m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno.EBADF, \u001b[33m\"\u001b[39m\u001b[33mread() on write-only GzipFile object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/_compression.py:68\u001b[39m, in \u001b[36mDecompressReader.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view.cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/gzip.py:547\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    545\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buf == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCompressed file ended before the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    548\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mend-of-stream marker was reached\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    550\u001b[39m \u001b[38;5;28mself\u001b[39m._crc = zlib.crc32(uncompress, \u001b[38;5;28mself\u001b[39m._crc)\n\u001b[32m    551\u001b[39m \u001b[38;5;28mself\u001b[39m._stream_size += \u001b[38;5;28mlen\u001b[39m(uncompress)\n",
      "\u001b[31mEOFError\u001b[39m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('./oxford-iiit-pet.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a14d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "DATA_DIR = './data/oxford-iiit-pet/images'\n",
    "\n",
    "files = os.listdir(DATA_DIR)\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_breed(fname):\n",
    "    parts = fname.split('_')\n",
    "    return ' '.join(parts[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_breed(files[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def open_image(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d752d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(open_image(os.path.join(DATA_DIR, files[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16dd57",
   "metadata": {},
   "source": [
    "### Creating a Custom PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "class PetsDataset(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.files = [fname for fname in os.listdir(root) if fname.endswith('.jpg')]\n",
    "        self.classes = list(set(parse_breed(fname) for fname in files))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        fname = self.files[i]\n",
    "        fpath = os.path.join(self.root, fname)\n",
    "        img = self.transform(open_image(fpath))\n",
    "        class_idx = self.classes.index(parse_breed(fname))\n",
    "        return img, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "img_size = 224\n",
    "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "dataset = PetsDataset(DATA_DIR, T.Compose([T.Resize(img_size),\n",
    "                                           T.Pad(8, padding_mode='reflect'),\n",
    "                                           T.RandomCrop(img_size),\n",
    "                                           T.ToTensor(),\n",
    "                                           T.Normalize(*imagenet_stats)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce47bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def denormalize(images, means, stds):\n",
    "    if len(images.shape) == 3:\n",
    "        images = images.unsqueeze(0)\n",
    "    means = torch.tensor(means).reshape(1,3,1,1)\n",
    "    stds = torch.tensor(stds).reshape(1,3,1,1)\n",
    "    return images * stds + means\n",
    "\n",
    "def show_image(img_tensor, label):\n",
    "    print('Label: ',dataset.classes[label], '(' + str(label) + ')')\n",
    "    img_tensor = denormalize(img_tensor, *imagenet_stats)[0].permute((1,2,0))\n",
    "    plt.imshow(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(*dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d8a67",
   "metadata": {},
   "source": [
    "### Creating Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9450c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pct = 0.1\n",
    "val_size = int(val_pct * len(dataset))\n",
    "\n",
    "train_ds, valid_ds = random_split(dataset, [len(dataset) - val_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2285ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaee3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 256\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(16,16))\n",
    "        ax.set_xticks([]), ax.set_yticks([])\n",
    "        images = denormalize(images[:64], *imagenet_stats)\n",
    "        ax.imshow(make_grid(images, nrow=8).permute(1,2,0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e124059",
   "metadata": {},
   "source": [
    "### Modifying a PreTrained Model (ResNet34)\n",
    "\n",
    "- Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c51eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch  \n",
    "        out = self(images)                      # Generate Predictions\n",
    "        loss = F.cross_entropy(out, labels)     # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                      # Generate Predictions\n",
    "        loss = F.cross_entropy(out, labels)     # Calculate loss \n",
    "        acc = accuracy(out, labels)             # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc':acc}\n",
    "    \n",
    "    def validation_epoch_end(self,outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()       # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc =  torch.stack(batch_accs).mean()         # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], {} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else\n",
    "            result['train_loss'], result['val_loss'], result['val_acc'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b52df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class PetsModel(ImageClassificationBase):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        # Use a Pretrained Model\n",
    "        self.network = models.resnet34(pretrained=pretrained)\n",
    "        # Replace Last Layer\n",
    "        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee9f659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNet34_Weights.DEFAULT\n",
    "resnet = models.resnet34(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f8d74",
   "metadata": {},
   "source": [
    "### GPU Tools and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b6c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63be94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation Phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "\n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,steps_per_epoch=len(train_loader))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation Phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e539eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6142e5e",
   "metadata": {},
   "source": [
    "### Finetuning the PreTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96714cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PetsModel(len(dataset.classes))\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7d7e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                         grad_clip=grad_clip,\n",
    "                         weight_decay=weight_decay,\n",
    "                         opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b3e67",
   "metadata": {},
   "source": [
    "### Training a model from Scratch\n",
    "\n",
    "Let's repeat the training without using weights from the pretrained ResNet34 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = PetsModel(len(dataset.classes), pretrained=False)\n",
    "to_device(model2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = [evaluate(model2, valid_dl)]\n",
    "history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50013d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 += fit_one_cycle(epochs, max_lr, model2, train_dl, valid_dl,\n",
    "                          grad_clip=grad_clip,\n",
    "                          weight_decay=weight_decay,\n",
    "                          opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946adba",
   "metadata": {},
   "source": [
    "While the pretrained model reached an accuracy of 80% in less than 3 minutes, the model without pretrained weights could only reach an accuracy of 24%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0a9ac",
   "metadata": {},
   "source": [
    "Function to see all the models available below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01631145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'AlexNet_Weights',\n",
       " 'ConvNeXt',\n",
       " 'ConvNeXt_Base_Weights',\n",
       " 'ConvNeXt_Large_Weights',\n",
       " 'ConvNeXt_Small_Weights',\n",
       " 'ConvNeXt_Tiny_Weights',\n",
       " 'DenseNet',\n",
       " 'DenseNet121_Weights',\n",
       " 'DenseNet161_Weights',\n",
       " 'DenseNet169_Weights',\n",
       " 'DenseNet201_Weights',\n",
       " 'EfficientNet',\n",
       " 'EfficientNet_B0_Weights',\n",
       " 'EfficientNet_B1_Weights',\n",
       " 'EfficientNet_B2_Weights',\n",
       " 'EfficientNet_B3_Weights',\n",
       " 'EfficientNet_B4_Weights',\n",
       " 'EfficientNet_B5_Weights',\n",
       " 'EfficientNet_B6_Weights',\n",
       " 'EfficientNet_B7_Weights',\n",
       " 'EfficientNet_V2_L_Weights',\n",
       " 'EfficientNet_V2_M_Weights',\n",
       " 'EfficientNet_V2_S_Weights',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'GoogLeNet_Weights',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'Inception_V3_Weights',\n",
       " 'MNASNet',\n",
       " 'MNASNet0_5_Weights',\n",
       " 'MNASNet0_75_Weights',\n",
       " 'MNASNet1_0_Weights',\n",
       " 'MNASNet1_3_Weights',\n",
       " 'MaxVit',\n",
       " 'MaxVit_T_Weights',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'MobileNet_V2_Weights',\n",
       " 'MobileNet_V3_Large_Weights',\n",
       " 'MobileNet_V3_Small_Weights',\n",
       " 'RegNet',\n",
       " 'RegNet_X_16GF_Weights',\n",
       " 'RegNet_X_1_6GF_Weights',\n",
       " 'RegNet_X_32GF_Weights',\n",
       " 'RegNet_X_3_2GF_Weights',\n",
       " 'RegNet_X_400MF_Weights',\n",
       " 'RegNet_X_800MF_Weights',\n",
       " 'RegNet_X_8GF_Weights',\n",
       " 'RegNet_Y_128GF_Weights',\n",
       " 'RegNet_Y_16GF_Weights',\n",
       " 'RegNet_Y_1_6GF_Weights',\n",
       " 'RegNet_Y_32GF_Weights',\n",
       " 'RegNet_Y_3_2GF_Weights',\n",
       " 'RegNet_Y_400MF_Weights',\n",
       " 'RegNet_Y_800MF_Weights',\n",
       " 'RegNet_Y_8GF_Weights',\n",
       " 'ResNeXt101_32X8D_Weights',\n",
       " 'ResNeXt101_64X4D_Weights',\n",
       " 'ResNeXt50_32X4D_Weights',\n",
       " 'ResNet',\n",
       " 'ResNet101_Weights',\n",
       " 'ResNet152_Weights',\n",
       " 'ResNet18_Weights',\n",
       " 'ResNet34_Weights',\n",
       " 'ResNet50_Weights',\n",
       " 'ShuffleNetV2',\n",
       " 'ShuffleNet_V2_X0_5_Weights',\n",
       " 'ShuffleNet_V2_X1_0_Weights',\n",
       " 'ShuffleNet_V2_X1_5_Weights',\n",
       " 'ShuffleNet_V2_X2_0_Weights',\n",
       " 'SqueezeNet',\n",
       " 'SqueezeNet1_0_Weights',\n",
       " 'SqueezeNet1_1_Weights',\n",
       " 'SwinTransformer',\n",
       " 'Swin_B_Weights',\n",
       " 'Swin_S_Weights',\n",
       " 'Swin_T_Weights',\n",
       " 'Swin_V2_B_Weights',\n",
       " 'Swin_V2_S_Weights',\n",
       " 'Swin_V2_T_Weights',\n",
       " 'VGG',\n",
       " 'VGG11_BN_Weights',\n",
       " 'VGG11_Weights',\n",
       " 'VGG13_BN_Weights',\n",
       " 'VGG13_Weights',\n",
       " 'VGG16_BN_Weights',\n",
       " 'VGG16_Weights',\n",
       " 'VGG19_BN_Weights',\n",
       " 'VGG19_Weights',\n",
       " 'ViT_B_16_Weights',\n",
       " 'ViT_B_32_Weights',\n",
       " 'ViT_H_14_Weights',\n",
       " 'ViT_L_16_Weights',\n",
       " 'ViT_L_32_Weights',\n",
       " 'VisionTransformer',\n",
       " 'Weights',\n",
       " 'WeightsEnum',\n",
       " 'Wide_ResNet101_2_Weights',\n",
       " 'Wide_ResNet50_2_Weights',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_api',\n",
       " '_meta',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'list_models',\n",
       " 'maxvit',\n",
       " 'maxvit_t',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_transformer',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL Env",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
